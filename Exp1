from google.colab import drive
drive.mount('/content/drive')

# Load the dataset from your Google Drive
import pandas as pd
file_path = '/content/drive/My Drive/MachineLearning/transactions.csv'  # adjust this path
df = pd.read_csv(file_path)

# Show first few rows
df.head()

# Explore dataset structure
print("Data Info:")
print(df.info())

# Summary statistics
print("\nData Description:")
print(df.describe())

# Check missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Fill missing numerical values with mean
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
imputer_num = SimpleImputer(strategy='mean')
df[num_cols] = imputer_num.fit_transform(df[num_cols])

# Fill missing categorical values with mode
cat_cols = df.select_dtypes(include=['object']).columns
imputer_cat = SimpleImputer(strategy='most_frequent')
df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])

print("\nMissing values after imputation:")
print(df.isnull().sum())

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd


# Label encode ordinal features (e.g., payment method if there's order)
if 'payment_method' in df.columns:
    le = LabelEncoder()
    df['payment_method'] = le.fit_transform(df['payment_method'])

# One-Hot encode nominal features
# Identify nominal columns excluding 'nameOrig', 'nameDest' and the target column
nominal_cols = df.select_dtypes(include='object').columns.tolist()
target_column = 'isFraud' if 'isFraud' in df.columns else None

if target_column and target_column in nominal_cols:
  nominal_cols.remove(target_column)

# Exclude 'nameOrig', 'nameDest' from nominal columns as they are identifiers
nominal_cols = [col for col in nominal_cols if col not in ['nameOrig', 'nameDest']]

df = pd.get_dummies(df, columns=nominal_cols, drop_first=True)


# ------------------------------
# 4. Scale numerical features

# Choose columns to scale (all numerical columns except 'isFraud' if it exists)
scale_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if target_column and target_column in scale_cols:
  scale_cols.remove(target_column)


# StandardScaler (good if outliers)
scaler = StandardScaler()
df[scale_cols] = scaler.fit_transform(df[scale_cols])

# ------------------------------
# 5. Train-Test Split

# Define target and features
if target_column and target_column in df.columns:
    X = df.drop(target_column, axis=1)
    y = df[target_column]
else:
    X = df.copy()
    y = None

# Split only if target is defined
if y is not None:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print("\nTrain/Test shapes:")
    print("X_train:", X_train.shape)
    print("X_test:", X_test.shape)
    print("y_train:", y_train.shape)
    print("y_test:", y_test.shape)
else:
    print("\nNo target column found; skipping train-test split.")

# ------------------------------
# 6. Final dataset
print("\nPreprocessed dataset preview:")
print(df.head())
